{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting E:/Dataset/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting E:/Dataset/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting E:/Dataset/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting E:/Dataset/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "ok!\n"
     ]
    }
   ],
   "source": [
    "location = 'E:/Dataset/MNIST/'\n",
    "mnist = input_data.read_data_sets(train_dir=location,one_hot=True)\n",
    "print('ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参数\n",
    "learning_rate = 0.01\n",
    "num_step = 500\n",
    "batch_size = 128\n",
    "display_step = 20\n",
    "\n",
    "\n",
    "#网络参数\n",
    "input_size = 784 # 28 X 28 图片\n",
    "num_class = 10  # MNIST类别为 0-9 共10类\n",
    "drop_out =0.75 #随机关断75%的神经元\n",
    "\n",
    "\n",
    "#tensorflow 图结构的输入\n",
    "X = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32,[None,num_class])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个卷积函数\n",
    "def conv2d(x,W,b,strides =1):\n",
    "    conv_result = tf.nn.conv2d(x,W,strides=[1,strides,strides,1],padding='SAME')\n",
    "    add_bias = tf.nn.bias_add(conv_result,b)\n",
    "    return tf.nn.relu(add_bias)\n",
    "\n",
    "def maxpooling2d(x,k=2):\n",
    "    #最大池化,k x k 池化核，步长为k\n",
    "    return tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')\n",
    "\n",
    "#搭建卷积神经网络\n",
    "def convolution_net(x,weights,biases,dropout_parame):\n",
    "    \n",
    "    # 将784  reshape 为  【batch_size, height ,width, channel】\n",
    "    x_reshaped = tf.reshape(x,shape=[-1,28,28,1])\n",
    "    \n",
    "#---------------第1个卷积池化--------------------------------------------  \n",
    "    conv1 = conv2d(x_reshaped,weights['wc1'],biases['bc1'])#卷积层\n",
    "    pooling1 = maxpooling2d(conv1,k=2) #池化层\n",
    "#-----------------------------------------------------------  \n",
    "    \n",
    "\n",
    "#---------------第2个卷积池化--------------------------------------------  \n",
    "    conv2 = conv2d(pooling1,weights['wc2'],biases['bc2'])#卷积层\n",
    "    pooling2 = maxpooling2d(conv2,k=2) #池化层\n",
    "#-----------------------------------------------------------  \n",
    "    \n",
    "#---------------全连接层---------------------------\n",
    "#     Reshape conv2 output to fit fully connected layer input，调整conv2层输出的结果以符合全连接层的需求\n",
    "    fc = tf.reshape(pooling2,[-1,weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc,weights['wd1']),biases['bd1'])\n",
    "    fc1_2 = tf.nn.relu(fc1)\n",
    "    fc_layer = tf.nn.dropout(fc1_2,dropout_parame) #dropout\n",
    "    \n",
    "    #输出层\n",
    "    out = tf.add(tf.matmul(fc_layer,weights['out']),biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网络参数\n",
    "weights = {\n",
    "     # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1':tf.Variable(tf.random_normal([5,5,1,32]),dtype=tf.float32),\n",
    "     # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2':tf.Variable(tf.random_normal([5,5,32,64]),dtype=tf.float32),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1':tf.Variable(tf.random_normal([7*7*64,1024]),dtype=tf.float32),\n",
    "     # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_class]),dtype=tf.float32)\n",
    "}\n",
    "biases = {\n",
    "     #对第一层每个卷积神经元的输出加上偏执，32个\n",
    "    'bc1':tf.Variable(tf.random_normal([32]),dtype=tf.float32),\n",
    "     #对第一层每个卷积神经元的输出加上偏执，64个\n",
    "    'bc2':tf.Variable(tf.random_normal([64]),dtype=tf.float32),\n",
    "     #对全连接层加上偏执，1024个\n",
    "    'bd1':tf.Variable(tf.random_normal([1024]),dtype=tf.float32),\n",
    "     #1024输入，num_class 个输出\n",
    "    'out':tf.Variable(tf.random_normal([num_class]),dtype=tf.float32)\n",
    "}\n",
    "\n",
    "#构建模型\n",
    "logits  = convolution_net(X,weights,biases,keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "#定义损失函数和优化函数\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss_op)\n",
    "\n",
    "#定义进行模型评估的方法\n",
    "correctct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctct_prediction,tf.float32))\n",
    "\n",
    "#初始化\n",
    "init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0,Minihatch loss: 113789.7969  Training Accuracy :0.1328\n",
      "step 20,Minihatch loss: 3473.7319  Training Accuracy :0.6719\n",
      "step 40,Minihatch loss: 655.5287  Training Accuracy :0.8672\n",
      "step 60,Minihatch loss: 487.5071  Training Accuracy :0.8750\n",
      "step 80,Minihatch loss: 638.9553  Training Accuracy :0.8906\n",
      "step 100,Minihatch loss: 585.3985  Training Accuracy :0.8359\n",
      "step 120,Minihatch loss: 478.7036  Training Accuracy :0.8828\n",
      "step 140,Minihatch loss: 174.8081  Training Accuracy :0.9062\n",
      "step 160,Minihatch loss: 354.0918  Training Accuracy :0.8984\n",
      "step 180,Minihatch loss: 211.2562  Training Accuracy :0.9453\n",
      "step 200,Minihatch loss: 177.2878  Training Accuracy :0.9453\n",
      "step 220,Minihatch loss: 246.1749  Training Accuracy :0.8984\n",
      "step 240,Minihatch loss: 167.8315  Training Accuracy :0.9531\n",
      "step 260,Minihatch loss: 112.7510  Training Accuracy :0.9609\n",
      "step 280,Minihatch loss: 183.6189  Training Accuracy :0.9219\n",
      "step 300,Minihatch loss: 103.7153  Training Accuracy :0.9531\n",
      "step 320,Minihatch loss: 183.3821  Training Accuracy :0.9297\n",
      "step 340,Minihatch loss: 236.4728  Training Accuracy :0.9219\n",
      "step 360,Minihatch loss: 98.0509  Training Accuracy :0.9531\n",
      "step 380,Minihatch loss: 85.9395  Training Accuracy :0.9453\n",
      "step 400,Minihatch loss: 104.7780  Training Accuracy :0.9453\n",
      "step 420,Minihatch loss: 140.3712  Training Accuracy :0.9297\n",
      "step 440,Minihatch loss: 51.2733  Training Accuracy :0.9453\n",
      "step 460,Minihatch loss: 66.1169  Training Accuracy :0.9453\n",
      "step 480,Minihatch loss: 40.4210  Training Accuracy :0.9609\n",
      "\n",
      "Optimization Finished !\n",
      "\n",
      "Test Accuracy : 0.96875\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for steps in range(num_step):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train_step,feed_dict = {X:batch[0], y:batch[1],keep_prob:drop_out})\n",
    "        \n",
    "        if (steps % display_step) ==0:\n",
    "            loss,acc = sess.run([loss_op,accuracy],feed_dict = { X:batch[0],y:batch[1],keep_prob:drop_out})\n",
    "            print('step '+str(steps)+',Minihatch loss: '+ '{:.4f}'.format(loss) + \\\n",
    "                 '  Training Accuracy :'+ '{:.4f}'.format(acc))\n",
    "    print('\\nOptimization Finished !\\n')\n",
    "    print('Test Accuracy : '+ str(sess.run(accuracy,feed_dict={ X : mnist.test.images[:256],\n",
    "                                                                y : mnist.test.labels[:256],\n",
    "                                                                keep_prob:drop_out})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
